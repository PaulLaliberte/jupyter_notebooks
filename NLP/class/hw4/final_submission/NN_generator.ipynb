{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import numpy as np\n",
    "import keras.layers as layers\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Reshape, Bidirectional, CuDNNLSTM, LSTM, SimpleRNN, Conv1D, MaxPooling1D, Flatten, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gold_standard(t, name):\n",
    "\n",
    "    with open(name + '.txt', 'w') as f:\n",
    "        for arr in t:\n",
    "            arr.remove('<s>')\n",
    "            arr.remove('</s>')\n",
    "            for line in arr:\n",
    "                f.write(line)\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission(t, name):\n",
    "    with open(name + '.txt', 'w') as f:\n",
    "        for arr in t:\n",
    "            for tup in arr:\n",
    "                line = '\\t'.join(['1', tup[0], tup[1]])\n",
    "                line += '\\n'\n",
    "                f.write(line)\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(keys, predictions):\n",
    "    \"\"\" Given a stream of gold standard word/tag pairs and a stream of system pairs. Figure out the the recall, precision and F1 \"\"\"\n",
    "\n",
    "\n",
    "    goldStandardEntities = findEntities(taggedData(keys))     # get the entities in the gold standard\n",
    "    systemEntities = findEntities(taggedData(predictions))    # and the entities in the system output\n",
    "\n",
    "    numEntities = len(goldStandardEntities)                   # number of entities there should be\n",
    "    numReturned = len(systemEntities)                         # number actually tagged by system\n",
    "    numTruePositives = len(set.intersection(goldStandardEntities,systemEntities))    # number of those that were right\n",
    "\n",
    "    precision = float(numTruePositives)/numReturned\n",
    "    recall = float(numTruePositives)/numEntities\n",
    "    f1 = 2 * (precision * recall)/(precision + recall)\n",
    "\n",
    "    print(numEntities, \" entities in gold standard.\")\n",
    "    print(numReturned, \" total entities found.\")\n",
    "    print(numTruePositives, \" of which were correct.\")\n",
    "    \n",
    "    print(\"Precision: \", precision, \"Recall: \", recall, \"F1-measure: \", f1)\n",
    "\n",
    "def findEntities(data):\n",
    "    \"\"\" Find all the IOB delimited entities in the data.  Return as a set of (begin, end) tuples. Data is sequence of word, tag pairs. \"\"\"\n",
    "\n",
    "    entities = set()\n",
    "\n",
    "    entityStart = 0\n",
    "    entityEnd = 0\n",
    "    \n",
    "    currentState = \"Q0\"\n",
    "    count = 0\n",
    "\n",
    "    for arr in list(data):\n",
    "        for word, tag in arr.items():\n",
    "            count = count + 1\n",
    "            if currentState == \"Q0\":\n",
    "                if tag == 'B':\n",
    "                    currentState = \"Q1\"\n",
    "                    entityStart = count\n",
    "            elif currentState == \"Q1\":\n",
    "                if tag == \"B\":\n",
    "                    entityEnd = count - 1\n",
    "                    entities.add((entityStart, entityEnd))\n",
    "                    entityStart = count\n",
    "                if tag == \"O\":\n",
    "                    entityEnd = count - 1\n",
    "                    entities.add((entityStart, entityEnd))\n",
    "                    currentState = \"Q0\"\n",
    "\n",
    "    if currentState == \"Q1\":\n",
    "        entities.add((entityStart, entityEnd))\n",
    "\n",
    "    return entities\n",
    "\n",
    "def taggedData(file):\n",
    "    for line in file:\n",
    "        if line.strip() == '':\n",
    "            yield({'</s>': 'O'})\n",
    "        else:\n",
    "            spl = line.strip().split()[1:]\n",
    "            yield({spl[0] : spl[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "s = []\n",
    "\n",
    "with open('../../data/gene-trainF17.txt') as f:\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            s.append(line)\n",
    "        else:\n",
    "            sentences.append(s)\n",
    "            s = []\n",
    "            \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i].insert(0, '<s>')\n",
    "    sentences[i].append('</s>')\n",
    "    \n",
    "train, test = train_test_split(sentences, test_size=.20, random_state=2)\n",
    "\n",
    "gold_test = copy.deepcopy(test)\n",
    "\n",
    "for arr in test:\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] != '<s>' and arr[i] != '</s>':\n",
    "            spl = arr[i].split('\\t')\n",
    "            arr[i] = '\\t'.join((spl[0], spl[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = []\n",
    "\n",
    "for arr in train:\n",
    "    tmp = []\n",
    "    for i in arr:\n",
    "        if i != '<s>' and i != '</s>':\n",
    "            line = i.replace('\\n', '').split('\\t')\n",
    "            tmp.append((line[1], line[2]))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    new_train.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'O'),\n",
       " ('confirm', 'O'),\n",
       " ('the', 'O'),\n",
       " ('binding', 'O'),\n",
       " ('of', 'O'),\n",
       " ('protein', 'O'),\n",
       " ('to', 'O'),\n",
       " ('these', 'O'),\n",
       " ('sites', 'O'),\n",
       " ('in', 'O'),\n",
       " ('cells', 'O'),\n",
       " (',', 'O'),\n",
       " ('we', 'O'),\n",
       " ('carried', 'O'),\n",
       " ('out', 'O'),\n",
       " ('an', 'O'),\n",
       " ('in', 'O'),\n",
       " ('vivo', 'O'),\n",
       " ('genomic', 'O'),\n",
       " ('footprinting', 'O'),\n",
       " ('analysis', 'O'),\n",
       " ('of', 'O'),\n",
       " ('this', 'O'),\n",
       " ('portion', 'O'),\n",
       " ('of', 'O'),\n",
       " ('the', 'O'),\n",
       " ('TGF', 'B'),\n",
       " ('alpha', 'I'),\n",
       " ('promoter', 'I'),\n",
       " ('in', 'O'),\n",
       " ('normal', 'O'),\n",
       " ('and', 'O'),\n",
       " ('transformed', 'O'),\n",
       " ('rat', 'O'),\n",
       " ('liver', 'O'),\n",
       " ('epithelial', 'O'),\n",
       " ('cell', 'O'),\n",
       " ('lines', 'O'),\n",
       " ('that', 'O'),\n",
       " ('express', 'O'),\n",
       " ('the', 'O'),\n",
       " ('endogenous', 'O'),\n",
       " ('gene', 'O'),\n",
       " ('at', 'O'),\n",
       " ('varying', 'O'),\n",
       " ('levels', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = []\n",
    "\n",
    "for arr in test:\n",
    "    tmp = []\n",
    "    for i in arr:\n",
    "        if i != '<s>' and i != '</s>':\n",
    "            tmp.append(i.replace('\\n', '').split('\\t')[1])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    new_test.append(' '.join(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The results showed that MIP , MMIF , FIC , Wimax , P0 . 1 and minute ventilation ( Vr ) were significantly increased after administration of methylphenidatum and aminophylline .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(word):\n",
    "    if re.match('[A-Z][a-z]+$', word):\n",
    "        shape = 'capital_letter'\n",
    "    elif re.match('[A-Z]+$', word):\n",
    "        shape = 'uppercase'\n",
    "    elif re.match('[a-z]+$', word):\n",
    "        shape = 'lowercase'\n",
    "    elif re.match('[A-Z][a-z]+[A-Z][a-z]+[A-Za-z]*$', word):\n",
    "        shape = 'camelcase'\n",
    "    elif re.match('[A-Za-z]+$', word):\n",
    "        shape = 'mixedcase'\n",
    "    elif re.match('\\W+$', word):\n",
    "        shape = 'punc'\n",
    "    elif re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "        shape = 'number'\n",
    "    elif re.match('[A-Za-z0-9]+\\.[A-Za-z0-9\\.]+\\.$', word):\n",
    "        shape = 'abbrev'\n",
    "    elif re.match('[A-Za-z0-9]+\\-[A-Za-z0-9\\-]+.*$', word):\n",
    "        shape = 'contains_hyphen'\n",
    "    elif re.match('[A-Za-z0-9]+\\.$', word):\n",
    "        shape = 'word_dot'\n",
    "    elif re.match('__.+__$', word):\n",
    "        shape = 'other'\n",
    "    else:\n",
    "        shape = 'unk'\n",
    " \n",
    "    return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sent, index, look_back):\n",
    "    #construct a feature template\n",
    "    index += 2\n",
    "    \n",
    "    sent = [('<s2>', '<s2>'), ('<s1>', '<s1>')] + sent + [('</s1>', '</s1>'), ('</s2>', '</s2>')]\n",
    "    look_back = ['<s1>', '<s1>'] + look_back\n",
    "    \n",
    "    word = sent[index][0]\n",
    "    n_word = sent[index + 1][0]\n",
    "    nn_word = sent[index + 2][0]\n",
    "    p_word = sent[index - 1][0]\n",
    "    pp_word = sent[index - 2][0]\n",
    "    p_iob = look_back[-1][0]\n",
    "    pp_iob = look_back[-2][0]\n",
    " \n",
    "    template = {\n",
    "            'word': word, 'stem': stemmer.stem(word), 'shape': get_shape(word),\n",
    "            'n_word': n_word, 'n_stem': stemmer.stem(n_word), 'n_shape': get_shape(n_word),\n",
    "            'nn_word': nn_word, 'nn_stem': stemmer.stem(nn_word), 'nn_shape': get_shape(nn_word),\n",
    "            'p_word': p_word, 'p_stem': stemmer.stem(p_word), 'p_iob': p_iob, 'p_shape': get_shape(p_word),\n",
    "            'pp_word': pp_word, 'pp_stem': stemmer.stem(pp_word), 'pp_iob': pp_iob, 'pp_shape': get_shape(pp_word)\n",
    "    }\n",
    "    \n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifer:\n",
    "    def __init__(self):\n",
    "        self._clf = self.build_nn()\n",
    "        self._dvec = DictVectorizer(sparse=False)\n",
    "        self._onehot = LabelBinarizer()\n",
    "        \n",
    "    def build_nn(self):\n",
    "        #FF w/ LSTM end\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2000, activation='relu', input_dim=10989))\n",
    "        model.add(RepeatVector(20))\n",
    "        model.add(CuDNNLSTM(50))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        \n",
    "        \"\"\"\n",
    "        #Convolutional NN\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10989, input_dim=10989))\n",
    "        model.add(RepeatVector(10))\n",
    "        model.add(Conv1D(111, kernel_size=4, strides=4, \n",
    "                         activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1000, activation='relu'))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop', metrics=['acc'])\n",
    "        model.summary()\n",
    "        \"\"\"\n",
    "        return model\n",
    "                \n",
    "    def online_training(self, train, batch_size):\n",
    "        #limit batch size to 1000\n",
    "        assert batch_size <= 1000\n",
    "        self._onehot.fit(['O', 'B', 'I'])\n",
    "        \n",
    "        first_batch = True\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        for i in tqdm_notebook(range(len(train)//batch_size - 1)):\n",
    "            sentences = train[start:end]\n",
    "            \n",
    "            X, y = [], []\n",
    "            for tagged in sentences:\n",
    "                iob_tags = [i[1] for i in tagged]\n",
    "            \n",
    "                for index in range(len(tagged)):\n",
    "                    X.append(get_features(tagged, index, iob_tags[:index]))\n",
    "                    encode = self._onehot.transform([iob_tags[index]])\n",
    "                    y.append(encode[0])\n",
    "                    \n",
    "            if first_batch == True:\n",
    "                self._dvec.fit(X)\n",
    "                X = self._dvec.transform(X)\n",
    "                self._clf.train_on_batch(X, np.array(y))\n",
    "                \n",
    "                first_batch = False\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "            else:\n",
    "                X = self._dvec.transform(X)\n",
    "                self._clf.train_on_batch(X, np.array(y))\n",
    "                \n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "        \n",
    "            print('Upper batch index: ', end)\n",
    "        \n",
    "        #train on last batch\n",
    "        print('Last batch start index: ', start)\n",
    "        print('Number of samples in last batch: ', len(train[start:]))\n",
    "        sentences = train[start:]\n",
    "            \n",
    "        X, y = [], []\n",
    "        for tagged in sentences:\n",
    "            iob_tags = [i[1] for i in tagged]\n",
    "            \n",
    "            for index in range(len(tagged)):\n",
    "                X.append(get_features(tagged, index, iob_tags[:index]))\n",
    "                encode = self._onehot.transform([iob_tags[index]])\n",
    "                y.append(encode[0])\n",
    "                    \n",
    "        X = self._dvec.transform(X)\n",
    "            \n",
    "        self._clf.train_on_batch(X, np.array(y))\n",
    "          \n",
    "    def predict_test(self, sentences):\n",
    "        look_back = []\n",
    "        predictions = []\n",
    "        \n",
    "        for arr in tqdm_notebook(sentences):\n",
    "            iob_tagged_tokens = []\n",
    "            for index, word in list(enumerate(arr)):\n",
    "                feat = self._dvec.transform(get_features(arr, index, look_back))\n",
    "                iob_tag = self._onehot.inverse_transform(self._clf.predict(feat))\n",
    "                history.append(iob_tag[0])\n",
    "                iob_tagged_tokens.append((word[0], iob_tag[0]))\n",
    "                \n",
    "            predictions.append(iob_tagged_tokens)\n",
    " \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2000)              21980000  \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 20, 2000)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 50)                410400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 22,390,553\n",
      "Trainable params: 22,390,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d255f142ae4cc6b26a067391f92238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=109), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper batch index:  200\n",
      "Upper batch index:  300\n",
      "Upper batch index:  400\n",
      "Upper batch index:  500\n",
      "Upper batch index:  600\n",
      "Upper batch index:  700\n",
      "Upper batch index:  800\n",
      "Upper batch index:  900\n",
      "Upper batch index:  1000\n",
      "Upper batch index:  1100\n",
      "Upper batch index:  1200\n",
      "Upper batch index:  1300\n",
      "Upper batch index:  1400\n",
      "Upper batch index:  1500\n",
      "Upper batch index:  1600\n",
      "Upper batch index:  1700\n",
      "Upper batch index:  1800\n",
      "Upper batch index:  1900\n",
      "Upper batch index:  2000\n",
      "Upper batch index:  2100\n",
      "Upper batch index:  2200\n",
      "Upper batch index:  2300\n",
      "Upper batch index:  2400\n",
      "Upper batch index:  2500\n",
      "Upper batch index:  2600\n",
      "Upper batch index:  2700\n",
      "Upper batch index:  2800\n",
      "Upper batch index:  2900\n",
      "Upper batch index:  3000\n",
      "Upper batch index:  3100\n",
      "Upper batch index:  3200\n",
      "Upper batch index:  3300\n",
      "Upper batch index:  3400\n",
      "Upper batch index:  3500\n",
      "Upper batch index:  3600\n",
      "Upper batch index:  3700\n",
      "Upper batch index:  3800\n",
      "Upper batch index:  3900\n",
      "Upper batch index:  4000\n",
      "Upper batch index:  4100\n",
      "Upper batch index:  4200\n",
      "Upper batch index:  4300\n",
      "Upper batch index:  4400\n",
      "Upper batch index:  4500\n",
      "Upper batch index:  4600\n",
      "Upper batch index:  4700\n",
      "Upper batch index:  4800\n",
      "Upper batch index:  4900\n",
      "Upper batch index:  5000\n",
      "Upper batch index:  5100\n",
      "Upper batch index:  5200\n",
      "Upper batch index:  5300\n",
      "Upper batch index:  5400\n",
      "Upper batch index:  5500\n",
      "Upper batch index:  5600\n",
      "Upper batch index:  5700\n",
      "Upper batch index:  5800\n",
      "Upper batch index:  5900\n",
      "Upper batch index:  6000\n",
      "Upper batch index:  6100\n",
      "Upper batch index:  6200\n",
      "Upper batch index:  6300\n",
      "Upper batch index:  6400\n",
      "Upper batch index:  6500\n",
      "Upper batch index:  6600\n",
      "Upper batch index:  6700\n",
      "Upper batch index:  6800\n",
      "Upper batch index:  6900\n",
      "Upper batch index:  7000\n",
      "Upper batch index:  7100\n",
      "Upper batch index:  7200\n",
      "Upper batch index:  7300\n",
      "Upper batch index:  7400\n",
      "Upper batch index:  7500\n",
      "Upper batch index:  7600\n",
      "Upper batch index:  7700\n",
      "Upper batch index:  7800\n",
      "Upper batch index:  7900\n",
      "Upper batch index:  8000\n",
      "Upper batch index:  8100\n",
      "Upper batch index:  8200\n",
      "Upper batch index:  8300\n",
      "Upper batch index:  8400\n",
      "Upper batch index:  8500\n",
      "Upper batch index:  8600\n",
      "Upper batch index:  8700\n",
      "Upper batch index:  8800\n",
      "Upper batch index:  8900\n",
      "Upper batch index:  9000\n",
      "Upper batch index:  9100\n",
      "Upper batch index:  9200\n",
      "Upper batch index:  9300\n",
      "Upper batch index:  9400\n",
      "Upper batch index:  9500\n",
      "Upper batch index:  9600\n",
      "Upper batch index:  9700\n",
      "Upper batch index:  9800\n",
      "Upper batch index:  9900\n",
      "Upper batch index:  10000\n",
      "Upper batch index:  10100\n",
      "Upper batch index:  10200\n",
      "Upper batch index:  10300\n",
      "Upper batch index:  10400\n",
      "Upper batch index:  10500\n",
      "Upper batch index:  10600\n",
      "Upper batch index:  10700\n",
      "Upper batch index:  10800\n",
      "Upper batch index:  10900\n",
      "Upper batch index:  11000\n",
      "\n",
      "Last batch start index:  10900\n",
      "Number of samples in last batch:  136\n"
     ]
    }
   ],
   "source": [
    "model = RNNClassifer()\n",
    "\n",
    "model.online_training(new_train, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The',),\n",
       " ('results',),\n",
       " ('showed',),\n",
       " ('that',),\n",
       " ('MIP',),\n",
       " (',',),\n",
       " ('MMIF',),\n",
       " (',',),\n",
       " ('FIC',),\n",
       " (',',),\n",
       " ('Wimax',),\n",
       " (',',),\n",
       " ('P0',),\n",
       " ('.',),\n",
       " ('1',),\n",
       " ('and',),\n",
       " ('minute',),\n",
       " ('ventilation',),\n",
       " ('(',),\n",
       " ('Vr',),\n",
       " (')',),\n",
       " ('were',),\n",
       " ('significantly',),\n",
       " ('increased',),\n",
       " ('after',),\n",
       " ('administration',),\n",
       " ('of',),\n",
       " ('methylphenidatum',),\n",
       " ('and',),\n",
       " ('aminophylline',),\n",
       " ('.',)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_test = [[(i,) for i in arr.split()] for arr in new_test]\n",
    "prep_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de50dbf66b884f168d96ea076ce9aa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2759), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict_test(prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_gold_standard(gold_test, 'gold_nng')\n",
    "write_submission(predictions, 'submit_keras_nng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = open('gold_nng.txt')\n",
    "predictions = open('submit_keras_nng.txt')\n",
    "eval(keys, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.replace('\\n', '').split('\\t')[1:] for i in gold_test[0] if i != '<s>' and i !='</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(x_0), max(x_1), #(9002, 25480, 9002)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
