{'memory': None, 'features__counts__encoding': 'utf-8', 'features__counts__binary': False, 'features__counts__token_pattern': '(?u)\\b\\w\\w+\\b', 'classifier__class_prior': None, 'features__counts__preprocessor': None, 'features__counts__max_df': 25, 'features__counts__norm': 'l2', 'features__counts__smooth_idf': True, 'classifier': MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True), 'features__counts__stop_words': None, 'features__n_jobs': 1, 'features__counts__decode_error': 'strict', 'features__counts__min_df': 2, 'features__transformer_weights': None, 'features': FeatureUnion(n_jobs=1,
       transformer_list=[('counts', Snowball_TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=25, max_features=None,
             min_df=2, ngram_range=(1, 3), norm='l2', pr...se, token_pattern='(?u)\\b\\w\\w+\\b',
             tokenizer=None, use_idf=True, vocabulary=None))],
       transformer_weights=None), 'features__counts__sublinear_tf': False, 'features__counts__ngram_range': (1, 3), 'features__counts__strip_accents': None, 'features__transformer_list': [('counts', Snowball_TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=25, max_features=None,
             min_df=2, ngram_range=(1, 3), norm='l2', preprocessor=None,
             smooth_idf=True, stop_words=None, strip_accents=None,
             sublinear_tf=False, token_pattern='(?u)\\b\\w\\w+\\b',
             tokenizer=None, use_idf=True, vocabulary=None))], 'features__counts__tokenizer': None, 'features__counts__lowercase': True, 'classifier__fit_prior': True, 'features__counts__max_features': None, 'features__counts__use_idf': True, 'classifier__alpha': 0.3, 'features__counts__analyzer': 'word', 'features__counts': Snowball_TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=25, max_features=None,
             min_df=2, ngram_range=(1, 3), norm='l2', preprocessor=None,
             smooth_idf=True, stop_words=None, strip_accents=None,
             sublinear_tf=False, token_pattern='(?u)\\b\\w\\w+\\b',
             tokenizer=None, use_idf=True, vocabulary=None), 'features__counts__dtype': <class 'numpy.int64'>, 'steps': [('features', FeatureUnion(n_jobs=1,
       transformer_list=[('counts', Snowball_TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=25, max_features=None,
             min_df=2, ngram_range=(1, 3), norm='l2', pr...se, token_pattern='(?u)\\b\\w\\w+\\b',
             tokenizer=None, use_idf=True, vocabulary=None))],
       transformer_weights=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))], 'features__counts__vocabulary': None, 'features__counts__input': 'content'}