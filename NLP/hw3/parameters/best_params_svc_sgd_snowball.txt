{'classifier__fit_intercept': True, 'classifier__alpha': 0.001, 'classifier__class_weight': None, 'features__counts__tokenizer': None, 'features__counts__ngram_range': (1, 2), 'features__n_jobs': 1, 'features__counts__analyzer': 'word', 'classifier__max_iter': 5000, 'features__transformer_weights': None, 'classifier__penalty': 'l2', 'features__counts__max_df': 20, 'features': FeatureUnion(n_jobs=1,
       transformer_list=[('counts', Snowball_CountVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=20, max_features=None,
             min_df=1, ngram_range=(1, 2), preprocessor=None,
             stop_words=None, strip_accents=None,
             token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None,
             vocabulary=None))],
       transformer_weights=None), 'features__counts__encoding': 'utf-8', 'features__transformer_list': [('counts', Snowball_CountVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=20, max_features=None,
             min_df=1, ngram_range=(1, 2), preprocessor=None,
             stop_words=None, strip_accents=None,
             token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None,
             vocabulary=None))], 'features__counts__min_df': 1, 'classifier__random_state': None, 'features__counts__decode_error': 'strict', 'classifier__learning_rate': 'optimal', 'classifier__power_t': 0.5, 'classifier__eta0': 0.001, 'features__counts__preprocessor': None, 'features__counts__dtype': <class 'numpy.int64'>, 'classifier__verbose': 0, 'features__counts__vocabulary': None, 'steps': [('features', FeatureUnion(n_jobs=1,
       transformer_list=[('counts', Snowball_CountVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=20, max_features=None,
             min_df=1, ngram_range=(1, 2), preprocessor=None,
             stop_words=None, strip_accents=None,
             token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None,
             vocabulary=None))],
       transformer_weights=None)), ('classifier', SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', max_iter=5000, n_iter=None,
       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
       shuffle=True, tol=None, verbose=0, warm_start=False))], 'classifier__loss': 'hinge', 'classifier__shuffle': True, 'classifier__epsilon': 0.1, 'features__counts__binary': False, 'features__counts__lowercase': True, 'features__counts__max_features': None, 'features__counts': Snowball_CountVectorizer(analyzer='word', binary=False, decode_error='strict',
             dtype=<class 'numpy.int64'>, encoding='utf-8',
             input='content', lowercase=True, max_df=20, max_features=None,
             min_df=1, ngram_range=(1, 2), preprocessor=None,
             stop_words=None, strip_accents=None,
             token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None,
             vocabulary=None), 'features__counts__token_pattern': '(?u)\\b\\w\\w+\\b', 'memory': None, 'classifier__warm_start': False, 'classifier__tol': None, 'classifier__l1_ratio': 0.15, 'classifier': SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', max_iter=5000, n_iter=None,
       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
       shuffle=True, tol=None, verbose=0, warm_start=False), 'classifier__n_iter': None, 'features__counts__stop_words': None, 'features__counts__strip_accents': None, 'features__counts__input': 'content', 'classifier__n_jobs': 1, 'classifier__average': False}